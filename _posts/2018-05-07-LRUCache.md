---
layout: post
title: "Implmenting LRU Cache"
date: "2018-05-07"
slug: "Implement-LRU-Cache"
description: "Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: Get and Set."
category: 
  -Algorithm, Data Structure
# tags will also be used as html meta keywords.
tags:
  - Algorithm
  - Coding
  - Data Structure
show_meta: false
comments: true
mathjax: true
mermaind: true
gistembed: true
published: true
noindex: false
nofollow: false
# hide QR code, permalink block while printing.
hide_printmsg: false
# show post summary or full post in RSS feed.
summaryfeed: false
## for twitter summary card with squared image and page description or page excerpt:
# imagesummary: foo.png
## for twitter card with large image:
# imagefeature: "http://img.youtube.com/vi/VEIrQUXm_hY/0.jpg"
## for twitter video card: (active for this page)
videocredit: tedtalks
---
Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: **Get** and **Set**.  
**get(key)** - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.

**set(key, value)** - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.
<!--more-->

Implement an LRU cache is a very popular algorithm question asked in Coding Interview. I find it interesting and an important question because it tests your knowledge about linked list and hashing. At the same time, interviewer might also be looking if your proposed solution is optimized so that the overall complexity of **Get** and **Set** method is $$O(1)$$. Successfully implementing this algorithm in a given time constraint will definitely make a positive impact on the interviewer and will outshine your coding skill.

Here is my approach to solve the problem:

**Requirements :**

1. Cache should continue to add element as long as there is a space.
* Implement a strategy to keep track of the current size of the cache.
* we can use a linked list to keep the chain of the elements and a hash to map the key with the node so that *get* is called in $$O(1)$$ time.
2. Element should be added, via *set* call such that the latest element added is always added to the head of the list. Hence **Head** of the list refers to the **Most** recently used element and the **Tail** of the list would be the **Least** recently used element.
3. If element already exist in the list, algorithm should be able to bring the node to the front. This can be done in $$O(1)$$ time if our list is a double Linked List.
4. If the size of the list grows beyond cache limit, delete the *Tail* node from the list. To make this operation $$O(1)$$, maintain the *Tail* of the list.
4. For **Get** operation, if element does not exist, return -1. Else, return the value of the node. Again this operation is $$O(1)$$ as hash map contains the reference to the node of the key and node stores the value.
5. Whenever Node is accessed via **Get**, Node should always be marked as *Most* recently used and hence brought to the front of the list.

Following is the implementation in Python
{% highlight python %}
# Define a Node Class for our List. 
# A node will hold key, value and a link to next and 
# prev nodes. (Link List we create is double Linked List)

class Node:
    def __init__(self, key, value):
        self.key = key
        self.value = value
        self.next = None
        self.prev = None

# Now create a list composed of node.

class LRUList:
    def __init__(self):
        # Head will be the Most Recently used Node.
        self.head = None

        # Tail will be the Least Recently used Node.
        self.tail = None
      
        #Maintain Size of the list
        self.size = 0
    
    # Method to add Node to the front of the List
    # Making it most recently used.
    def push_front(self, node):
        
        node.next = self.head
        
        if self.head is not None:
            self.head.prev = node
            
        self.head = node
        
        if self.tail is None:
            self.tail = node
            
        self.size = self.size + 1

    # Method to delete a given node.
    # $$O(1)$$ operation as it is a double link list 
    def delete(self, node):
        if node.prev is not None:
            node.prev.next = node.next
        if node.next is not None:
            node.next.prev = node.prev
            
        if(self.head == node):
            self.head = node.next
        
        if(self.tail == node):
            self.tail = node.prev
        
        del node
        self.size = self.size - 1

class LRUCache:
    # LRUCache is composed of our LRUList and a dictionary mapping key to 
    # Node in the list.
    def __init__(self, capacity):
        self.size = capacity
        self.d = {}
        self.List = LRUList()

    # Method to get a value of the key.
    def get(self, key):
        if(key not in self.d):
            return -1
        
        val = self.d[key].value
        
        self.List.delete(self.d[key])
        
        n = Node(key, val)
        
        self.List.push_front(n)
        
        self.d[key] = n
        
        return val
        
    # Method to insert element into cache.
    def set(self, key, value):
        if(key in self.d):
            self.List.delete(self.d[key])
        
        n = Node(key, value)
        
        self.List.push_front(n)
        
        self.d[key] = n
        
        if(self.List.size > self.size) :
            tail = self.List.tail
            self.List.delete(tail)
            del self.d[tail.key]


{% endhighlight %}
